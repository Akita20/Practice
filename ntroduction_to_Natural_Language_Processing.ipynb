{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akita20/Practice/blob/master/ntroduction_to_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " lecture notes on Natural Language Processing (NLP) while focusing on the core concepts of stop words, tokenization, and examples in a Google Colab setting.\n",
        "\n",
        "**Title: Introduction to Natural Language Processing**\n",
        "\n",
        "**1. Stop Words**\n",
        "\n",
        "* **What are they?** Stop words are common words in a language that frequently appear in text but usually provide little to no relevant meaning to the core content.  Examples include \"the,\" \"a,\" \"an,\" \"is,\" \"and,\" etc.\n",
        "* **Why remove them?**\n",
        "    * **Efficiency:** Stop words take up space and processing time. Removing them makes your models more efficient to train.\n",
        "    * **Focus:** Filtering out stop words puts the emphasis on more content-rich, meaningful words.\n",
        "    * **Context:** In *some* cases, stop words may have significance for understanding context (e.g., sentiment analysis), so consider this carefully.\n",
        "\n",
        "**Example (Colab):**"
      ],
      "metadata": {
        "id": "26ALch0-A8nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')  # Download NLTK's stopwords database\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"The cat sat on the mat with a hat.\"\n",
        "filtered_words = [word for word in sentence.split() if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_words)  # Output: ['cat', 'sat', 'mat', 'hat']"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'sat', 'mat', 'hat.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPvloJYkA8nW",
        "outputId": "55831c3d-f089-4f05-d4bf-e39215597d88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tokenization**\n",
        "\n",
        "* **What is it?** Tokenization breaks down a piece of text into smaller units called tokens. These tokens can be:\n",
        "    * **Words:** \"This is a sample sentence.\" → [\"This\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
        "    * **Sentences:** \"This is a sample sentence. Here's another.\" → [\"This is a sample sentence.\", \"Here's another.\"]\n",
        "    * **Subwords:**  \"powerful\" → ['power', 'ful'] (useful for handling complex words)\n",
        "        \n",
        "* **Why do it?** Machine learning models can't understand raw text directly. Tokenization makes text into a structured format that algorithms can process.\n",
        "\n",
        "**Types of Tokenizers:**\n",
        "\n",
        "   * **Word Tokenizers** (e.g., NLTK's word_tokenize)\n",
        "   * **Sentence Tokenizers** (e.g., NLTK's sent_tokenize)\n",
        "   * **More advanced tokenizers** that handle things like contractions, special characters, and complex word forms.\n",
        "\n",
        "**Example (Colab):**"
      ],
      "metadata": {
        "id": "H6NADjSGA8nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Natural language processing (NLP) is fascinating!\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)  # Output: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'fascinating', '!']"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'fascinating', '!']\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPW7GYuiA8nY",
        "outputId": "38ef33e8-fd39-4d69-e8e0-2f301aa2924a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Steps in NLP (Colab)**"
      ],
      "metadata": {
        "id": "8wWKWLXUA8nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Text to Sequences\n",
        "\n",
        "**Why we do it:** Computers don't understand language the same way humans do. Most NLP models work with numerical data. Therefore, we need to convert our text into sequences of numbers for the model to process.\n",
        "\n",
        "**How it works:**\n",
        "1. **Building a Vocabulary:** A tokenizer (like the Keras Tokenizer) scans your text data and creates a vocabulary (essentially a dictionary) of unique words.\n",
        "2. **Assigning Integers:** Each unique word in the vocabulary is assigned a unique integer.\n",
        "3. **Text Transformation:** Your original text sentences are transformed into lists of integers, where each integer represents a corresponding word in the vocabulary.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Original sentence: \"The cat sat on the mat.\"\n",
        "\n",
        "Vocabulary: {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5}\n",
        "\n",
        "Integer sequence: [1, 2, 3, 4, 5]\n",
        "\n"
      ],
      "metadata": {
        "id": "xAICEZcjCOhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Text to Sequences\n",
        "\n",
        "**Core Idea:** Instead of processing raw text directly, NLP models typically operate on numerical representations of words. We convert sentences into sequences of integers where each integer is a unique code for a word within our vocabulary.\n",
        "\n",
        "**Why Do We Do This?** Computers excel at number manipulation, not at directly deciphering human language.\n"
      ],
      "metadata": {
        "id": "4shnI5HaEKDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization\n",
        "\n",
        "**Explanation:**\n",
        "1. The tokenizer builds a vocabulary (a mapping of words to unique integers).\n",
        "2. Each sentence is split into words.\n",
        "3. Each word is replaced by its corresponding integer code from the vocabulary.\n"
      ],
      "metadata": {
        "id": "BchdmDEcERlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"Dogs are playful and loyal.\"\n",
        "]"
      ],
      "metadata": {
        "id": "vs7Nka3zHpXo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a basic tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)  # Build vocabulary based on the text"
      ],
      "metadata": {
        "id": "jD3SUXKrHtWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6OfmHnCHu6j",
        "outputId": "782cfece-d0de-49a7-8f4e-fc7bf8ab1ed4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'cat': 2,\n",
              " 'sat': 3,\n",
              " 'on': 4,\n",
              " 'mat': 5,\n",
              " 'dogs': 6,\n",
              " 'are': 7,\n",
              " 'playful': 8,\n",
              " 'and': 9,\n",
              " 'loyal': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4TDeAdFH_3_",
        "outputId": "f9b31d51-4439-4079-9923-9ec8536f4d2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4, 1, 5], [6, 7, 8, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Padding Sequences\n",
        "\n",
        "**Core Idea:** Many neural network models, particularly architectures like GRUs and RNNs, expect inputs to have a consistent length. Padding achieves this by adding neutral tokens (usually zeros) to shorter sequences.\n",
        "\n",
        "**Why Do This?** Ensuring sequences have the same length allows models to process them in batches, optimizing learning and computation.\n"
      ],
      "metadata": {
        "id": "oFjHZq_bHitJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding Sequences\n",
        "\n",
        "**Why we do it:** Many NLP models, particularly those using neural networks like Recurrent Neural Networks (RNNs) or GRUs, are designed to work with fixed-length inputs. But, sentences in real-world text data are naturally of varying lengths. Padding resolves this issue.\n",
        "\n",
        "**How it works:**\n",
        "1. **Choosing a maximum length:** You decide a maximum length (e.g., 20 words).\n",
        "2. **Shorter sequences:** Sequences shorter than the maximum length are padded with zeros (or a special padding token) at the beginning or end to reach the desired length.\n",
        "3. **Longer sequences:** Sequences longer than the maximum length are truncated to fit.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Sentences:\n",
        "- \"The dog barked loudly.\"\n",
        "- \"The sun is shining.\"\n",
        "\n",
        "Maximum Length: 6\n",
        "\n",
        "Padded Sequences:\n",
        "- [0, 1, 4, 5, 6, 7] ('0' represents padding)\n",
        "- [1, 8, 2, 9, 0, 0]\n"
      ],
      "metadata": {
        "id": "ycYkT25OHXd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Our sequences from the previous example\n",
        "sequences = [[1, 2, 3, 4, 5], [6, 7, 8, 9]]\n",
        "\n",
        "# Set the desired maximum length\n",
        "maxlen = 6\n",
        "\n",
        "# Apply padding\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "print(padded_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT3mf3gQEWqJ",
        "outputId": "bc379ae6-82dc-4772-f4d9-441c0e4fd110"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 2 3 4 5]\n",
            " [0 0 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/Ilovecodingforever/EECS-487-Humor-Classification\">https://github.com/Ilovecodingforever/EECS-487-Humor-Classification</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "kfASGoWiA8na"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}